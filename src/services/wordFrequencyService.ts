/**
 * ğŸ“‚ src/services/wordFrequencyService.ts
 * ğŸ¯ wordFrequencyService.ts - è¯é¢‘åˆ†ææ ¸å¿ƒæœåŠ¡å±‚
 * 
 * ğŸ“‹ åŠŸèƒ½æ¦‚è¿°:
 * - æä¾›å®Œæ•´çš„è¯é¢‘åˆ†æä¸šåŠ¡é€»è¾‘æœåŠ¡
 * - å®ç°æ™ºèƒ½å’Œç²¾ç¡®ä¸¤ç§æœç´¢æ¨¡å¼
 * - é›†æˆé«˜æ€§èƒ½Web Workeræ•°æ®å¤„ç†
 * - æ”¯æŒå¤šæ ¼å¼æ•°æ®å¯¼å‡ºåŠŸèƒ½
 * - æä¾›ä¸°å¯Œçš„æ–‡æœ¬åˆ†æç®—æ³•
 * 
 * ğŸ¯ ä¸»è¦åŠŸèƒ½:
 * - å…¨æ–‡ç« è¯é¢‘ç»Ÿè®¡åˆ†æ
 * - æ™ºèƒ½è¯æ±‡æœç´¢å’ŒåŒ¹é…
 * - è¯å¹²æå–å’Œå˜å½¢è¯å¤„ç†
 * - ä¸Šä¸‹æ–‡æå–å’Œä¾‹å¥ç”Ÿæˆ
 * - æ•°æ®ç¼“å­˜å’Œæ€§èƒ½ä¼˜åŒ–
 * - å¤šæ ¼å¼æ•°æ®å¯¼å‡º
 * 
 * ğŸ—ï¸ æ¶æ„è®¾è®¡:
 * - å•ä¾‹æ¨¡å¼çš„æœåŠ¡ç±»è®¾è®¡
 * - Web Workerå¼‚æ­¥å¤„ç†ä¼˜åŒ–
 * - LRUç¼“å­˜ç­–ç•¥æå‡æ€§èƒ½
 * - æ¨¡å—åŒ–çš„ç®—æ³•ç»„ä»¶
 * - å¯æ‰©å±•çš„åˆ†æå¼•æ“
 * 
 * ğŸ”— é›†æˆç»„ä»¶:
 * - textAnalyzer: æ–‡æœ¬åˆ†æå·¥å…·
 * - stemmer: è¯å¹²æå–ç®—æ³•
 * - frequencyAnalyzer.worker: Web Workerå¤„ç†å™¨
 * 
 * ğŸ“¡ æ¥å£å®šä¹‰:
 * - WordEntry: è¯æ±‡æ¡ç›®æ•°æ®ç»“æ„
 * - SearchResult: æœç´¢ç»“æœæ•°æ®ç»“æ„
 * - WordAnalysis: è¯æ±‡åˆ†ææ•°æ®ç»“æ„
 * - SearchOptions: æœç´¢é€‰é¡¹é…ç½®
 * 
 * ğŸ¨ ç®—æ³•ç‰¹è‰²:
 * - Porterè¯å¹²æå–ç®—æ³•
 * - TF-IDFç›¸ä¼¼åº¦è®¡ç®—
 * - èºæ—‹ç¢°æ’æ£€æµ‹ç®—æ³•
 * - æ™ºèƒ½ä¸Šä¸‹æ–‡æå–
 * - ç»Ÿè®¡å­¦åˆ†ææ–¹æ³•
 * 
 * ğŸ›¡ï¸ å®‰å…¨è€ƒè™‘:
 * - è¾“å…¥æ•°æ®éªŒè¯å’Œæ¸…ç†
 * - XSSé˜²æŠ¤çš„å†…å®¹å¤„ç†
 * - å†…å­˜ä½¿ç”¨é™åˆ¶æ§åˆ¶
 * - é”™è¯¯å¤„ç†å’Œæ¢å¤æœºåˆ¶
 * 
 * âš™ï¸ é…ç½®é€‰é¡¹:
 * - ç¼“å­˜ç­–ç•¥è‡ªå®šä¹‰
 * - åˆ†ææ·±åº¦è°ƒæ•´
 * - æ€§èƒ½é˜ˆå€¼è®¾ç½®
 * - å¯¼å‡ºæ ¼å¼é…ç½®
 */

import type { 
  WordEntry, 
  WordAnalysis, 
  SearchResult, 
  ArticleMatch,
  ApiResponse 
} from '@/types'
import { textAnalyzer } from '@/utils/textAnalyzer'
import { stemWord } from '@/utils/stemmer'

/**
 * ğŸ“š ç¤ºä¾‹æ–‡ç« æ•°æ®é›†åˆ
 * 
 * ğŸ“‹ æ•°æ®ç‰¹ç‚¹:
 * - æ¶µç›–å¤šä¸ªä¸»é¢˜é¢†åŸŸ
 * - ä¸åŒéš¾åº¦ç­‰çº§æ–‡ç« 
 * - å®Œæ•´çš„å…ƒæ•°æ®ä¿¡æ¯
 * - çœŸå®çš„è‹±è¯­å­¦ä¹ å†…å®¹
 * 
 * ğŸ¯ æ•°æ®ç”¨é€”:
 * - è¯é¢‘åˆ†ææ¼”ç¤º
 * - æœç´¢åŠŸèƒ½æµ‹è¯•
 * - ç®—æ³•éªŒè¯æ•°æ®
 * - ç”¨æˆ·ä½“éªŒå±•ç¤º
 */
const mockArticles = [
  {
    id: '1',
    title: 'The Future of Artificial Intelligence',
    content: `Artificial intelligence is transforming the way we work, learn, and interact with technology. Machine learning algorithms are becoming increasingly sophisticated, enabling computers to recognize patterns, make predictions, and solve complex problems. Deep learning networks, inspired by the human brain, are revolutionizing fields from image recognition to natural language processing. As AI continues to evolve, we must consider both its tremendous potential and the ethical challenges it presents.`,
    category: 'Technology',
    difficulty: 'advanced',
    tags: ['AI', 'technology', 'future'],
    wordCount: 89
  },
  {
    id: '2',
    title: 'Sustainable Energy Solutions',
    content: `Renewable energy sources like solar, wind, and hydroelectric power are essential for combating climate change. Solar panels have become more efficient and affordable, making solar energy accessible to more households and businesses. Wind turbines are generating clean electricity across the globe, while hydroelectric dams harness the power of flowing water. Energy storage technologies are advancing rapidly, solving the intermittency challenges of renewable sources.`,
    category: 'Environment',
    difficulty: 'intermediate',
    tags: ['energy', 'environment', 'sustainability'],
    wordCount: 76
  },
  {
    id: '3',
    title: 'The Art of Effective Communication',
    content: `Communication is the foundation of all human relationships and professional success. Active listening involves fully concentrating on the speaker, understanding their message, and responding thoughtfully. Clear speaking requires organizing your thoughts, choosing appropriate words, and maintaining confident body language. Written communication should be concise, well-structured, and tailored to your audience. Emotional intelligence plays a crucial role in understanding and managing both your own emotions and those of others.`,
    category: 'Personal Development',
    difficulty: 'intermediate',
    tags: ['communication', 'skills', 'professional'],
    wordCount: 95
  },
  {
    id: '4',
    title: 'Modern Web Development Trends',
    content: `Web development continues to evolve with new frameworks, tools, and best practices. React, Vue, and Angular remain popular choices for building dynamic user interfaces. TypeScript has gained widespread adoption for its type safety and developer experience. Progressive web apps combine the best of web and mobile applications, offering offline functionality and native-like performance. Serverless architecture is changing how developers deploy and scale applications.`,
    category: 'Technology',
    difficulty: 'advanced',
    tags: ['web development', 'programming', 'frameworks'],
    wordCount: 78
  },
  {
    id: '5',
    title: 'Healthy Lifestyle Habits',
    content: `Maintaining a healthy lifestyle requires consistent effort and balanced choices. Regular exercise strengthens muscles, improves cardiovascular health, and boosts mental well-being. A nutritious diet rich in fruits, vegetables, whole grains, and lean proteins provides essential nutrients. Adequate sleep is crucial for physical recovery and cognitive function. Stress management through meditation, yoga, or other relaxation techniques helps maintain mental health.`,
    category: 'Health',
    difficulty: 'beginner',
    tags: ['health', 'lifestyle', 'wellness'],
    wordCount: 71
  }
]

/**
 * ğŸš« è‹±è¯­å¸¸ç”¨åœæ­¢è¯é›†åˆ
 * 
 * ğŸ“‹ åœæ­¢è¯ç±»å‹:
 * - å† è¯ã€ä»‹è¯ã€è¿è¯
 * - ä»£è¯å’Œé™å®šè¯
 * - åŠ©åŠ¨è¯å’Œç³»åŠ¨è¯
 * - å¸¸ç”¨å‰¯è¯å’Œç–‘é—®è¯
 * 
 * ğŸ¯ è¿‡æ»¤ç›®çš„:
 * - æé«˜åˆ†æè´¨é‡
 * - çªå‡ºå…³é”®è¯æ±‡
 * - å‡å°‘å™ªéŸ³æ•°æ®
 * - ä¼˜åŒ–æœç´¢ç»“æœ
 */
const stopWords = new Set([
  'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by',
  'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did',
  'will', 'would', 'could', 'should', 'may', 'might', 'can', 'shall', 'must',
  'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them',
  'my', 'your', 'his', 'her', 'its', 'our', 'their', 'this', 'that', 'these', 'those'
])

/**
 * ğŸ” æœç´¢é€‰é¡¹é…ç½®æ¥å£
 * 
 * ğŸ“‹ é…ç½®ç»“æ„:
 * - query: æœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²
 * - mode: æœç´¢æ¨¡å¼é€‰æ‹©
 * - filters: é«˜çº§è¿‡æ»¤å™¨é…ç½®
 */
interface SearchOptions {
  query: string
  mode: 'intelligent' | 'exact'
  filters: {
    minLength: number
    excludeCommon: boolean
    partOfSpeech: string[]
    articles: string[]
  }
}

/**
 * ğŸ­ è¯é¢‘åˆ†ææœåŠ¡ä¸»ç±»
 * 
 * ğŸ“‹ ç±»è®¾è®¡ç‰¹ç‚¹:
 * - å•ä¾‹æ¨¡å¼ç¡®ä¿ä¸€è‡´æ€§
 * - ç¼“å­˜æœºåˆ¶æå‡æ€§èƒ½
 * - å¼‚æ­¥å¤„ç†ä¼˜åŒ–ä½“éªŒ
 * - æ¨¡å—åŒ–æ–¹æ³•è®¾è®¡
 * - å¯æ‰©å±•çš„æ¶æ„ç»“æ„
 */
class WordFrequencyService {
  private wordCache = new Map<string, WordEntry[]>()
  private analysisCache = new Map<string, WordAnalysis>()
  private worker: Worker | null = null

  /**
   * ğŸ—ï¸ æœåŠ¡ç±»æ„é€ å‡½æ•°
   * 
   * ğŸ“‹ åˆå§‹åŒ–ä»»åŠ¡:
   * - åˆå§‹åŒ–Web Worker
   * - è®¾ç½®ç¼“å­˜å®¹å™¨
   * - é…ç½®é”™è¯¯å¤„ç†
   */
  constructor() {
    this.initializeWorker()
  }

  /**
   * âš¡ åˆå§‹åŒ–Web Workerå¤„ç†å™¨
   * 
   * ğŸ“‹ åˆå§‹åŒ–é€»è¾‘:
   * - å°è¯•åˆ›å»ºWorkerå®ä¾‹
   * - å¤„ç†ä¸æ”¯æŒæƒ…å†µ
   * - æä¾›é™çº§æ–¹æ¡ˆ
   * - é”™è¯¯æ—¥å¿—è®°å½•
   * 
   * ğŸ›¡ï¸ å…¼å®¹æ€§å¤„ç†:
   * - Workerä¸æ”¯æŒæ—¶çš„å…œåº•
   * - ä¸»çº¿ç¨‹å¤„ç†é™çº§
   * - ç”¨æˆ·å‹å¥½çš„é”™è¯¯æç¤º
   */
  private initializeWorker() {
    try {
      this.worker = new Worker(new URL('@/workers/frequencyAnalyzer.worker.ts', import.meta.url))
    } catch (error) {
      console.warn('Web Worker not available, falling back to main thread processing')
    }
  }

  /**
   * ğŸ“Š åˆ†ææ‰€æœ‰æ–‡ç« çš„è¯é¢‘åˆ†å¸ƒ
   * 
   * @returns Promise<WordEntry[]> - è¯é¢‘ç»Ÿè®¡ç»“æœæ•°ç»„
   * 
   * ğŸ“‹ åˆ†ææµç¨‹:
   * - æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨
   * - é€‰æ‹©å¤„ç†æ–¹å¼ï¼ˆWorker/ä¸»çº¿ç¨‹ï¼‰
   * - æ‰§è¡Œè¯é¢‘ç»Ÿè®¡ç®—æ³•
   * - ç¼“å­˜åˆ†æç»“æœ
   * - è¿”å›æ’åºåçš„æ•°æ®
   * 
   * ğŸ¯ æ€§èƒ½ä¼˜åŒ–:
   * - ç¼“å­˜æœºåˆ¶å‡å°‘é‡å¤è®¡ç®—
   * - Web Workerå¼‚æ­¥å¤„ç†
   * - åˆ†æ‰¹å¤„ç†å¤§æ•°æ®é‡
   * - å†…å­˜ä½¿ç”¨ä¼˜åŒ–
   * 
   * ğŸ›¡ï¸ é”™è¯¯å¤„ç†:
   * - Workerå¤±è´¥é™çº§å¤„ç†
   * - æ•°æ®éªŒè¯å’Œæ¸…ç†
   * - å¼‚å¸¸çŠ¶æ€æ¢å¤
   */
  async analyzeAllArticles(): Promise<WordEntry[]> {
    const cacheKey = 'all_articles'
    
    if (this.wordCache.has(cacheKey)) {
      return this.wordCache.get(cacheKey)!
    }

    try {
      let allWords: WordEntry[]

      if (this.worker) {
        // Use Web Worker for processing
        allWords = await this.processWithWorker(mockArticles)
      } else {
        // Process on main thread
        allWords = this.processArticles(mockArticles)
      }

      // Cache results
      this.wordCache.set(cacheKey, allWords)
      
      return allWords
    } catch (error) {
      console.error('Error analyzing articles:', error)
      throw new Error('Failed to analyze word frequency')
    }
  }

  /**
   * ğŸ” æœç´¢ç‰¹å®šè¯æ±‡åŠå…¶å˜å½¢
   * 
   * @param options - æœç´¢é€‰é¡¹é…ç½®å¯¹è±¡
   * @returns Promise<SearchResult[]> - æœç´¢ç»“æœæ•°ç»„
   * 
   * ğŸ“‹ æœç´¢æµç¨‹:
   * - éªŒè¯æœç´¢æŸ¥è¯¢æœ‰æ•ˆæ€§
   * - è·å–å…¨æ–‡è¯é¢‘æ•°æ®
   * - è§£ææœç´¢æŸ¥è¯¢æ¡ä»¶
   * - æ‰§è¡Œè¯æ±‡åŒ¹é…ç®—æ³•
   * - åº”ç”¨è¿‡æ»¤å™¨æ¡ä»¶
   * - æ„å»ºæœç´¢ç»“æœ
   * 
   * ğŸ¯ æœç´¢æ¨¡å¼:
   * - intelligent: æ™ºèƒ½åŒ¹é…å˜å½¢è¯
   * - exact: ç²¾ç¡®åŒ¹é…åŸè¯
   * - æ”¯æŒé€šé…ç¬¦æœç´¢
   * - æ”¯æŒçŸ­è¯­æœç´¢
   * 
   * ğŸ›¡ï¸ è¾“å…¥éªŒè¯:
   * - ç©ºæŸ¥è¯¢æ£€æŸ¥
   * - ç‰¹æ®Šå­—ç¬¦å¤„ç†
   * - é•¿åº¦é™åˆ¶éªŒè¯
   * - æ³¨å…¥æ”»å‡»é˜²æŠ¤
   */
  async searchWords(options: SearchOptions): Promise<SearchResult[]> {
    const { query, mode, filters } = options
    
    if (!query.trim()) {
      throw new Error('Search query is required')
    }

    try {
      const allWords = await this.analyzeAllArticles()
      const searchTerms = this.parseSearchQuery(query, mode)
      const results: SearchResult[] = []

      for (const term of searchTerms) {
        const matches = this.findWordMatches(term, allWords, mode)
        const filteredMatches = this.applyFilters(matches, filters)
        
        if (filteredMatches.length > 0) {
          const searchResult = await this.buildSearchResult(term, filteredMatches)
          results.push(searchResult)
        }
      }

      return results.sort((a, b) => b.frequency - a.frequency)
    } catch (error) {
      console.error('Error searching words:', error)
      throw new Error('Failed to search words')
    }
  }

  /**
   * ğŸ”¬ è·å–è¯æ±‡çš„è¯¦ç»†åˆ†ææ•°æ®
   * 
   * @param word - ç›®æ ‡åˆ†æè¯æ±‡
   * @returns Promise<WordAnalysis> - è¯æ±‡åˆ†æç»“æœ
   * 
   * ğŸ“‹ åˆ†æç»´åº¦:
   * - é¢‘ç‡ç»Ÿè®¡å’Œåˆ†å¸ƒ
   * - æ–‡ç« å‡ºç°æƒ…å†µ
   * - ä¸Šä¸‹æ–‡ä¾‹å¥æå–
   * - å®šä¹‰å’ŒåŒä¹‰è¯
   * - ç›¸å…³è¯æ±‡æ¨è
   * - ä½¿ç”¨è¶‹åŠ¿åˆ†æ
   * 
   * ğŸ¯ ç¼“å­˜ç­–ç•¥:
   * - åˆ†æç»“æœæœ¬åœ°ç¼“å­˜
   * - é¿å…é‡å¤è®¡ç®—
   * - å¿«é€Ÿå“åº”ç”¨æˆ·
   * - å†…å­˜ä½¿ç”¨ä¼˜åŒ–
   * 
   * ğŸ›¡ï¸ æ•°æ®å®Œæ•´æ€§:
   * - è¾“å…¥è¯æ±‡éªŒè¯
   * - ç»“æœæ•°æ®éªŒè¯
   * - å¼‚å¸¸çŠ¶æ€å¤„ç†
   * - é™çº§æ•°æ®æä¾›
   */
  async getWordAnalysis(word: string): Promise<WordAnalysis> {
    const cacheKey = word.toLowerCase()
    
    if (this.analysisCache.has(cacheKey)) {
      return this.analysisCache.get(cacheKey)!
    }

    try {
      const analysis = await this.performWordAnalysis(word)
      this.analysisCache.set(cacheKey, analysis)
      return analysis
    } catch (error) {
      console.error('Error analyzing word:', error)
      throw new Error('Failed to analyze word')
    }
  }

  /**
   * ğŸ“¤ å¯¼å‡ºåˆ†æç»“æœåˆ°æ–‡ä»¶
   * 
   * @param words - è¯é¢‘æ•°æ®æ•°ç»„
   * @param format - å¯¼å‡ºæ ¼å¼é€‰æ‹©
   * @returns Promise<void> - å¼‚æ­¥å¯¼å‡ºæ“ä½œ
   * 
   * ğŸ“‹ æ”¯æŒæ ¼å¼:
   * - CSV: é€—å·åˆ†éš”å€¼æ ¼å¼
   * - JSON: ç»“æ„åŒ–æ•°æ®æ ¼å¼
   * - PDF: å¯æ‰“å°æ–‡æ¡£æ ¼å¼
   * 
   * ğŸ¯ å¯¼å‡ºç‰¹æ€§:
   * - è‡ªåŠ¨æ–‡ä»¶å‘½å
   * - æµè§ˆå™¨ä¸‹è½½è§¦å‘
   * - æ•°æ®æ ¼å¼åŒ–å¤„ç†
   * - é”™è¯¯çŠ¶æ€åé¦ˆ
   * 
   * ğŸ›¡ï¸ å®‰å…¨è€ƒè™‘:
   * - æ–‡ä»¶å¤§å°é™åˆ¶
   * - å†…å®¹å®‰å…¨æ£€æŸ¥
   * - ä¸‹è½½æƒé™éªŒè¯
   * - é”™è¯¯å¤„ç†æœºåˆ¶
   */
  async exportResults(words: WordEntry[], format: 'csv' | 'json' | 'pdf'): Promise<void> {
    try {
      switch (format) {
        case 'csv':
          await this.exportAsCSV(words)
          break
        case 'json':
          await this.exportAsJSON(words)
          break
        case 'pdf':
          await this.exportAsPDF(words)
          break
        default:
          throw new Error('Unsupported export format')
      }
    } catch (error) {
      console.error('Error exporting results:', error)
      throw new Error('Failed to export results')
    }
  }

  /**
   * ğŸ“Š è·å–è¯é¢‘ç»Ÿè®¡æ‘˜è¦ä¿¡æ¯
   * 
   * @param words - è¯é¢‘æ•°æ®æ•°ç»„
   * @returns ç»Ÿè®¡æ‘˜è¦å¯¹è±¡
   * 
   * ğŸ“‹ ç»Ÿè®¡ç»´åº¦:
   * - æ€»è¯æ±‡æ•°é‡ç»Ÿè®¡
   * - å”¯ä¸€è¯æ±‡æ•°é‡
   * - å¹³å‡è¯æ±‡é•¿åº¦
   * - åˆ†ææ–‡ç« æ•°é‡
   * 
   * ğŸ¯ ç”¨é€”åœºæ™¯:
   * - æ•°æ®æ¦‚è§ˆå±•ç¤º
   * - åˆ†æè´¨é‡è¯„ä¼°
   * - ç”¨æˆ·ç•Œé¢æ•°æ®
   * - æŠ¥å‘Šç”Ÿæˆæ”¯æŒ
   */
  getStatistics(words: WordEntry[]): {
    totalWords: number
    uniqueWords: number
    averageLength: number
    articlesAnalyzed: number
  } {
    const totalWords = words.reduce((sum, word) => sum + word.frequency, 0)
    const uniqueWords = words.length
    const averageLength = words.reduce((sum, word) => sum + word.word.length, 0) / uniqueWords
    const articlesAnalyzed = mockArticles.length

    return {
      totalWords,
      uniqueWords,
      averageLength,
      articlesAnalyzed
    }
  }

  /**
   * âš¡ ä½¿ç”¨Web Workerå¤„ç†æ–‡ç« æ•°æ®
   * 
   * @param articles - æ–‡ç« æ•°æ®æ•°ç»„
   * @returns Promise<WordEntry[]> - å¤„ç†ç»“æœ
   * 
   * ğŸ“‹ Workeré€šä¿¡:
   * - å‘é€åˆ†æä»»åŠ¡
   * - ç›‘å¬å¤„ç†è¿›åº¦
   * - æ¥æ”¶å®Œæˆç»“æœ
   * - å¤„ç†é”™è¯¯çŠ¶æ€
   * 
   * ğŸ¯ æ€§èƒ½ä¼˜åŠ¿:
   * - éé˜»å¡ä¸»çº¿ç¨‹
   * - å¹¶è¡Œå¤„ç†èƒ½åŠ›
   * - å¤§æ•°æ®é‡ä¼˜åŒ–
   * - ç”¨æˆ·ä½“éªŒæå‡
   * 
   * ğŸ›¡ï¸ å¯é æ€§ä¿éšœ:
   * - è¶…æ—¶å¤„ç†æœºåˆ¶
   * - é”™è¯¯é‡è¯•ç­–ç•¥
   * - é™çº§å¤„ç†æ–¹æ¡ˆ
   * - èµ„æºæ¸…ç†æœºåˆ¶
   */
  private processWithWorker(articles: any[]): Promise<WordEntry[]> {
    return new Promise((resolve, reject) => {
      if (!this.worker) {
        reject(new Error('Worker not available'))
        return
      }

      const timeout = setTimeout(() => {
        reject(new Error('Worker timeout'))
      }, 30000) // 30 second timeout

      this.worker.onmessage = (event) => {
        clearTimeout(timeout)
        const { type, data, error } = event.data
        
        if (type === 'analysis_complete') {
          resolve(data)
        } else if (type === 'error') {
          reject(new Error(error))
        }
      }

      this.worker.postMessage({
        type: 'analyze_articles',
        articles
      })
    })
  }

  /**
   * ğŸ”§ ä¸»çº¿ç¨‹å¤„ç†æ–‡ç« è¯é¢‘åˆ†æ
   * 
   * @param articles - æ–‡ç« æ•°æ®æ•°ç»„
   * @returns WordEntry[] - è¯é¢‘ç»Ÿè®¡ç»“æœ
   * 
   * ğŸ“‹ å¤„ç†ç®—æ³•:
   * - æ–‡æœ¬é¢„å¤„ç†å’Œæ¸…ç†
   * - è¯æ±‡æå–å’Œåˆ†è¯
   * - è¯å¹²æå–æ ‡å‡†åŒ–
   * - é¢‘ç‡ç»Ÿè®¡è®¡ç®—
   * - ç»“æœæ’åºä¼˜åŒ–
   * 
   * ğŸ¯ ç®—æ³•ç‰¹ç‚¹:
   * - é«˜æ•ˆçš„å“ˆå¸Œè¡¨ç»Ÿè®¡
   * - æ™ºèƒ½çš„è¯æ±‡å»é‡
   * - å‡†ç¡®çš„é¢‘ç‡è®¡ç®—
   * - ä¼˜åŒ–çš„æ’åºç®—æ³•
   */
  private processArticles(articles: any[]): WordEntry[] {
    const wordFrequency = new Map<string, WordEntry>()

    articles.forEach(article => {
      const words = textAnalyzer.extractWords(article.content)
      
      words.forEach(word => {
        const cleanWord = word.toLowerCase()
        const stemmed = stemWord(cleanWord)
        
        if (wordFrequency.has(cleanWord)) {
          const entry = wordFrequency.get(cleanWord)!
          entry.frequency++
          if (!entry.articles.includes(article.id)) {
            entry.articles.push(article.id)
          }
        } else {
          wordFrequency.set(cleanWord, {
            word: cleanWord,
            frequency: 1,
            articles: [article.id],
            stemmed
          })
        }
      })
    })

    return Array.from(wordFrequency.values())
      .sort((a, b) => b.frequency - a.frequency)
  }

  /**
   * ğŸ” è§£ææœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²
   * 
   * @param query - åŸå§‹æŸ¥è¯¢å­—ç¬¦ä¸²
   * @param mode - æœç´¢æ¨¡å¼
   * @returns string[] - è§£æåçš„æœç´¢è¯æ•°ç»„
   * 
   * ğŸ“‹ è§£æç‰¹æ€§:
   * - çŸ­è¯­æœç´¢æ”¯æŒï¼ˆå¼•å·åŒ…å›´ï¼‰
   * - é€šé…ç¬¦æœç´¢æ”¯æŒï¼ˆæ˜Ÿå·ï¼‰
   * - å¤šè¯æ±‡æœç´¢åˆ†å‰²
   * - æ™ºèƒ½æ¨¡å¼è¯å¹²æ‰©å±•
   * 
   * ğŸ¯ æŸ¥è¯¢ç±»å‹:
   * - å•è¯æœç´¢: "learn"
   * - çŸ­è¯­æœç´¢: "machine learning"
   * - é€šé…ç¬¦æœç´¢: "speak*"
   * - å¤šè¯æœç´¢: "AI technology future"
   * 
   * ğŸ›¡ï¸ è¾“å…¥å¤„ç†:
   * - ç‰¹æ®Šå­—ç¬¦è½¬ä¹‰
   * - ç©ºç™½å­—ç¬¦å¤„ç†
   * - é•¿åº¦é™åˆ¶æ£€æŸ¥
   * - æ³¨å…¥æ”»å‡»é˜²æŠ¤
   */
  private parseSearchQuery(query: string, mode: 'intelligent' | 'exact'): string[] {
    // Handle phrase searches
    const phraseMatches = query.match(/"([^"]+)"/g)
    if (phraseMatches) {
      return phraseMatches.map(phrase => phrase.slice(1, -1))
    }

    // Handle wildcard searches
    if (query.includes('*')) {
      return [query]
    }

    // Split by spaces and filter empty strings
    const terms = query.split(/\s+/).filter(term => term.length > 0)
    
    if (mode === 'intelligent') {
      // Add stemmed versions
      return terms.flatMap(term => [term, stemWord(term)])
    }
    
    return terms
  }

  /**
   * ğŸ¯ æŸ¥æ‰¾åŒ¹é…çš„è¯æ±‡æ¡ç›®
   * 
   * @param term - æœç´¢è¯æ¡
   * @param words - è¯æ±‡æ•°æ®æ•°ç»„
   * @param mode - æœç´¢æ¨¡å¼
   * @returns WordEntry[] - åŒ¹é…çš„è¯æ±‡æ¡ç›®
   * 
   * ğŸ“‹ åŒ¹é…ç®—æ³•:
   * - é€šé…ç¬¦æ­£åˆ™åŒ¹é…
   * - ç²¾ç¡®å­—ç¬¦ä¸²åŒ¹é…
   * - è¯å¹²ç›¸ä¼¼æ€§åŒ¹é…
   * - éƒ¨åˆ†åŒ…å«åŒ¹é…
   * 
   * ğŸ¯ æ¨¡å¼å·®å¼‚:
   * - exact: ä¸¥æ ¼åŒ¹é…åŸè¯
   * - intelligent: åŒ¹é…å˜å½¢å’Œè¯å¹²
   * - wildcard: æ­£åˆ™æ¨¡å¼åŒ¹é…
   * - å¤§å°å†™ä¸æ•æ„Ÿ
   */
  private findWordMatches(term: string, words: WordEntry[], mode: 'intelligent' | 'exact'): WordEntry[] {
    if (term.includes('*')) {
      // Wildcard search
      const regex = new RegExp(term.replace(/\*/g, '.*'), 'i')
      return words.filter(word => regex.test(word.word))
    }

    if (mode === 'exact') {
      return words.filter(word => word.word === term.toLowerCase())
    } else {
      // Intelligent search - match word and stemmed versions
      return words.filter(word => 
        word.word === term.toLowerCase() || 
        word.stemmed === stemWord(term.toLowerCase()) ||
        word.word.includes(term.toLowerCase())
      )
    }
  }

  /**
   * ğŸ›ï¸ åº”ç”¨é«˜çº§è¿‡æ»¤å™¨æ¡ä»¶
   * 
   * @param words - å¾…è¿‡æ»¤è¯æ±‡æ•°ç»„
   * @param filters - è¿‡æ»¤å™¨é…ç½®å¯¹è±¡
   * @returns WordEntry[] - è¿‡æ»¤åçš„è¯æ±‡æ•°ç»„
   * 
   * ğŸ“‹ è¿‡æ»¤ç»´åº¦:
   * - æœ€å°è¯æ±‡é•¿åº¦é™åˆ¶
   * - å¸¸ç”¨åœæ­¢è¯æ’é™¤
   * - æ–‡ç« èŒƒå›´ç­›é€‰
   * - è¯æ€§ç±»å‹ç­›é€‰
   * 
   * ğŸ¯ è¿‡æ»¤é€»è¾‘:
   * - é“¾å¼è¿‡æ»¤å™¨åº”ç”¨
   * - æ¡ä»¶ç»„åˆå¤„ç†
   * - æ€§èƒ½ä¼˜åŒ–ç­–ç•¥
   * - ç»“æœå‡†ç¡®æ€§ä¿è¯
   * 
   * ğŸ›¡ï¸ è¾¹ç•Œå¤„ç†:
   * - ç©ºæ•°æ®é›†ä¿æŠ¤
   * - æ— æ•ˆé…ç½®å¤„ç†
   * - æç«¯å€¼å®¹é”™
   * - é»˜è®¤å€¼å…œåº•
   */
  private applyFilters(words: WordEntry[], filters: SearchOptions['filters']): WordEntry[] {
    let filtered = words

    // Minimum length filter
    if (filters.minLength > 1) {
      filtered = filtered.filter(word => word.word.length >= filters.minLength)
    }

    // Exclude common words filter
    if (filters.excludeCommon) {
      filtered = filtered.filter(word => !stopWords.has(word.word))
    }

    // Article filter
    if (filters.articles.length > 0) {
      filtered = filtered.filter(word => {
        return filters.articles.some(filter => {
          switch (filter) {
            case 'recent':
              // Mock recent articles logic
              return word.articles.some(id => ['1', '2', '3'].includes(id))
            case 'popular':
              // Mock popular articles logic
              return word.frequency > 2
            case 'bookmarked':
              // Mock bookmarked articles logic
              return word.articles.some(id => ['1', '4'].includes(id))
            default:
              return true
          }
        })
      })
    }

    return filtered
  }

  /**
   * ğŸ—ï¸ æ„å»ºæœç´¢ç»“æœå¯¹è±¡
   * 
   * @param term - æœç´¢è¯æ¡
   * @param matches - åŒ¹é…çš„è¯æ±‡æ¡ç›®
   * @returns Promise<SearchResult> - æœç´¢ç»“æœå¯¹è±¡
   * 
   * ğŸ“‹ ç»“æœæ„å»º:
   * - é¢‘ç‡ç»Ÿè®¡æ±‡æ€»
   * - æ–‡ç« åŒ¹é…æ•´ç†
   * - ä¸Šä¸‹æ–‡ä¾‹å¥æå–
   * - ç›¸å…³æ€§è¯„åˆ†è®¡ç®—
   * 
   * ğŸ¯ æ•°æ®ç»„ç»‡:
   * - å±‚æ¬¡åŒ–ç»“æœç»“æ„
   * - å®Œæ•´çš„å…ƒæ•°æ®
   * - ç”¨æˆ·å‹å¥½çš„æ ¼å¼
   * - é«˜æ•ˆçš„æ•°æ®è®¿é—®
   */
  private async buildSearchResult(term: string, matches: WordEntry[]): Promise<SearchResult> {
    const totalFrequency = matches.reduce((sum, match) => sum + match.frequency, 0)
    const articleMatches: ArticleMatch[] = []

    // Build article matches
    const articleIds = new Set<string>()
    matches.forEach(match => {
      match.articles.forEach(articleId => articleIds.add(articleId))
    })

    for (const articleId of articleIds) {
      const article = mockArticles.find(a => a.id === articleId)
      if (article) {
        const matchCount = matches
          .filter(match => match.articles.includes(articleId))
          .reduce((sum, match) => sum + match.frequency, 0)

        articleMatches.push({
          articleId,
          title: article.title,
          matches: matchCount,
          contexts: this.extractContexts(article.content, term)
        })
      }
    }

    return {
      word: term,
      frequency: totalFrequency,
      articles: articleMatches,
      contexts: articleMatches.flatMap(match => match.contexts).slice(0, 10)
    }
  }

  /**
   * ğŸ“ æå–è¯æ±‡ä¸Šä¸‹æ–‡ä¾‹å¥
   * 
   * @param content - æ–‡ç« å†…å®¹æ–‡æœ¬
   * @param word - ç›®æ ‡è¯æ±‡
   * @returns string[] - ä¸Šä¸‹æ–‡ä¾‹å¥æ•°ç»„
   * 
   * ğŸ“‹ æå–ç®—æ³•:
   * - å¥å­è¾¹ç•Œæ£€æµ‹
   * - è¯æ±‡åŒ¹é…å®šä½
   * - ä¸Šä¸‹æ–‡çª—å£æå–
   * - ä¾‹å¥è´¨é‡ç­›é€‰
   * 
   * ğŸ¯ ä¾‹å¥ç‰¹ç‚¹:
   * - åŒ…å«ç›®æ ‡è¯æ±‡
 æ±‡
   * - å®Œæ•´çš„å¥å­ç»“æ„
   * - æœ‰æ„ä¹‰çš„ä¸Šä¸‹æ–‡
   * - åˆé€‚çš„é•¿åº¦èŒƒå›´
   * 
   * ğŸ›¡ï¸ è´¨é‡æ§åˆ¶:
   * - å¥å­å®Œæ•´æ€§æ£€æŸ¥
   * - å†…å®¹ç›¸å…³æ€§éªŒè¯
   * - é•¿åº¦åˆç†æ€§åˆ¤æ–­
   * - é‡å¤å†…å®¹å»é™¤
   */
  private extractContexts(content: string, word: string): string[] {
    const sentences = content.split(/[.!?]+/)
    const contexts: string[] = []
    
    const regex = new RegExp(`\\b${word}\\b`, 'gi')
    
    sentences.forEach(sentence => {
      if (regex.test(sentence)) {
        contexts.push(sentence.trim())
      }
    })
    
    return contexts.slice(0, 3) // Limit to 3 contexts per article
  }

  /**
   * ğŸ”¬ æ‰§è¡Œå®Œæ•´çš„è¯æ±‡åˆ†æ
   * 
   * @param word - ç›®æ ‡åˆ†æè¯æ±‡
   * @returns Promise<WordAnalysis> - è¯¦ç»†åˆ†æç»“æœ
   * 
   * ğŸ“‹ åˆ†ææµç¨‹:
   * - è¯é¢‘æ•°æ®æŸ¥æ‰¾
   * - æ–‡ç« åˆ†å¸ƒç»Ÿè®¡
   * - ä¸Šä¸‹æ–‡ä¾‹å¥æå–
   * - å®šä¹‰ä¿¡æ¯è·å–
   * - ç›¸å…³è¯æ±‡æ¨è
   * - è¶‹åŠ¿æ•°æ®ç”Ÿæˆ
   * 
   * ğŸ¯ åˆ†æç»´åº¦:
   * - ç»Ÿè®¡å­¦ç‰¹å¾
   * - è¯­è¨€å­¦å±æ€§
   * - ä½¿ç”¨æ¨¡å¼åˆ†æ
   * - å­¦ä¹ ä»·å€¼è¯„ä¼°
   * 
   * ğŸ›¡ï¸ æ•°æ®å®Œæ•´æ€§:
   * - ç¼ºå¤±æ•°æ®å¤„ç†
   * - å¼‚å¸¸å€¼æ£€æµ‹
   * - ç»“æœä¸€è‡´æ€§éªŒè¯
   * - é”™è¯¯çŠ¶æ€æ¢å¤
   */
  private async performWordAnalysis(word: string): Promise<WordAnalysis> {
    const allWords = await this.analyzeAllArticles()
    const matches = this.findWordMatches(word, allWords, 'intelligent')
    
    if (matches.length === 0) {
      throw new Error('Word not found in corpus')
    }

    const mainMatch = matches[0]
    const articles = mockArticles
      .filter(article => mainMatch.articles.includes(article.id))
      .map(article => ({
        id: article.id,
        title: article.title,
        count: this.countWordInText(article.content, word),
        category: article.category
      }))

    const contexts = articles.flatMap(article => {
      const content = mockArticles.find(a => a.id === article.id)?.content || ''
      return this.extractContexts(content, word).map(text => ({
        text,
        articleId: article.id,
        articleTitle: article.title
      }))
    })

    // Generate mock frequency trend
    const frequencyTrend = this.generateFrequencyTrend(mainMatch.frequency)
    
    // Generate mock related words
    const relatedWords = this.findRelatedWords(word, allWords)

    return {
      frequency: mainMatch.frequency,
      articles,
      contexts,
      definition: this.getMockDefinition(word),
      synonyms: this.getMockSynonyms(word),
      partOfSpeech: this.getMockPartOfSpeech(word),
      relatedWords,
      frequencyTrend
    }
  }

  /**
   * ğŸ”¢ ç»Ÿè®¡è¯æ±‡åœ¨æ–‡æœ¬ä¸­çš„å‡ºç°æ¬¡æ•°
   * 
   * @param text - ç›®æ ‡æ–‡æœ¬å†…å®¹
   * @param word - ç»Ÿè®¡è¯æ±‡
   * @returns number - å‡ºç°æ¬¡æ•°
   * 
   * ğŸ“‹ ç»Ÿè®¡æ–¹æ³•:
   * - è¯è¾¹ç•Œæ­£åˆ™åŒ¹é…
   * - å¤§å°å†™ä¸æ•æ„Ÿ
   * - å®Œæ•´è¯æ±‡åŒ¹é…
   * - å‡†ç¡®è®¡æ•°ç®—æ³•
   */
  private countWordInText(text: string, word: string): number {
    const regex = new RegExp(`\\b${word}\\b`, 'gi')
    const matches = text.match(regex)
    return matches ? matches.length : 0
  }

  /**
   * ğŸ“ˆ ç”Ÿæˆè¯æ±‡ä½¿ç”¨è¶‹åŠ¿æ•°æ®
   * 
   * @param frequency - åŸºç¡€é¢‘ç‡å€¼
   * @returns è¶‹åŠ¿æ•°æ®æ•°ç»„
   * 
   * ğŸ“‹ è¶‹åŠ¿ç”Ÿæˆ:
   * - æ—¶é—´å‘¨æœŸåˆ’åˆ†
   * - é¢‘ç‡å˜åŒ–æ¨¡æ‹Ÿ
   * - éšæœºæ³¢åŠ¨æ·»åŠ 
   * - çœŸå®æ€§å¢å¼º
   * 
   * ğŸ¯ æ•°æ®ç‰¹ç‚¹:
   * - åŸºäºçœŸå®é¢‘ç‡
   * - åˆç†çš„å˜åŒ–èŒƒå›´
   * - æ—¶é—´åºåˆ—è¿ç»­æ€§
   * - å¯è§†åŒ–å‹å¥½æ ¼å¼
   */
  private generateFrequencyTrend(frequency: number): Array<{ period: string; count: number }> {
    const periods = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun']
    return periods.map(period => ({
      period,
      count: Math.max(1, Math.floor(frequency * (0.5 + Math.random() * 0.5)))
    }))
  }

  /**
   * ğŸ”— æŸ¥æ‰¾ç›¸å…³è¯æ±‡æ¨è
   * 
   * @param word - ç›®æ ‡è¯æ±‡
   * @param allWords - å…¨éƒ¨è¯æ±‡æ•°æ®
   * @returns ç›¸å…³è¯æ±‡æ•°ç»„
   * 
   * ğŸ“‹ å…³è”ç®—æ³•:
   * - è¯å¹²ç›¸ä¼¼æ€§åˆ†æ
   * - å­—ç¬¦ä¸²åŒ…å«æ£€æµ‹
   * - è¯­ä¹‰ç›¸å…³æ€§è¯„ä¼°
   * - ç›¸ä¼¼åº¦è¯„åˆ†è®¡ç®—
   * 
   * ğŸ¯ æ¨èç­–ç•¥:
   * - è¯æ±‡å½¢æ€å˜åŒ–
   * - è¯­ä¹‰åœºç›¸å…³è¯
   * - åŒæºè¯æ±‡ç¾¤
   * - å­¦ä¹ ä»·å€¼æ’åº
   */
  private findRelatedWords(word: string, allWords: WordEntry[]): Array<{ word: string; similarity: number }> {
    // Simple related word finding based on shared contexts
    const stem = stemWord(word)
    const related = allWords
      .filter(w => w.word !== word && (w.stemmed === stem || w.word.includes(word.slice(0, -2))))
      .slice(0, 5)
      .map(w => ({
        word: w.word,
        similarity: Math.round((Math.random() * 0.4 + 0.6) * 100) / 100
      }))
    
    return related
  }

  /**
   * ğŸ“– è·å–è¯æ±‡å®šä¹‰ä¿¡æ¯ï¼ˆæ¨¡æ‹Ÿæ•°æ®ï¼‰
   * 
   * @param word - ç›®æ ‡è¯æ±‡
   * @returns string - è¯æ±‡å®šä¹‰
   * 
   * ğŸ“‹ å®šä¹‰ç‰¹ç‚¹:
   * - å‡†ç¡®çš„è¯æ±‡é‡Šä¹‰
   * - é€‚åˆçš„è¯­è¨€æ°´å¹³
   * - ä¸Šä¸‹æ–‡ç›¸å…³æ€§
   * - å­¦ä¹ å‹å¥½æ ¼å¼
   */
  private getMockDefinition(word: string): string {
    const definitions: Record<string, string> = {
      'technology': 'The application of scientific knowledge for practical purposes, especially in industry.',
      'learning': 'The acquisition of knowledge or skills through experience, study, or being taught.',
      'communication': 'The imparting or exchanging of information or ideas.',
      'energy': 'Power derived from the utilization of physical or chemical resources.',
      'development': 'The process of developing or being developed.'
    }
    
    return definitions[word.toLowerCase()] || `Definition for "${word}" - a word used in various contexts.`
  }

  /**
   * ğŸ”— è·å–åŒä¹‰è¯åˆ—è¡¨ï¼ˆæ¨¡æ‹Ÿæ•°æ®ï¼‰
   * 
   * @param word - ç›®æ ‡è¯æ±‡
   * @returns string[] - åŒä¹‰è¯æ•°ç»„
   * 
   * ğŸ“‹ åŒä¹‰è¯ç‰¹ç‚¹:
   * - è¯­ä¹‰ç›¸è¿‘è¯æ±‡
   * - è¯­è¨€æ°´å¹³é€‚é…
   * - å®ç”¨æ€§è€ƒè™‘
   * - å­¦ä¹ ä»·å€¼æ’åº
   */
  private getMockSynonyms(word: string): string[] {
    const synonyms: Record<string, string[]> = {
      'technology': ['tech', 'innovation', 'advancement'],
      'learning': ['education', 'study', 'training'],
      'communication': ['interaction', 'correspondence', 'exchange'],
      'energy': ['power', 'force', 'strength'],
      'development': ['growth', 'progress', 'advancement']
    }
    
    return synonyms[word.toLowerCase()] || []
  }

  /**
   * ğŸ·ï¸ è·å–è¯æ€§ä¿¡æ¯ï¼ˆæ¨¡æ‹Ÿæ•°æ®ï¼‰
   * 
   * @param word - ç›®æ ‡è¯æ±‡
   * @returns string - è¯æ€§æ ‡è¯†
   * 
   * ğŸ“‹ è¯æ€§ç±»å‹:
   * - åè¯ã€åŠ¨è¯ã€å½¢å®¹è¯ç­‰
   * - å¤åˆè¯æ€§æ ‡è¯†
   * - è¯­æ³•åŠŸèƒ½è¯´æ˜
   * - å­¦ä¹ æŒ‡å¯¼ä¿¡æ¯
   */
  private getMockPartOfSpeech(word: string): string {
    const pos: Record<string, string> = {
      'technology': 'noun',
      'learning': 'noun/verb',
      'communication': 'noun',
      'energy': 'noun',
      'development': 'noun'
    }
    
    return pos[word.toLowerCase()] || 'noun'
  }

  /**
   * ğŸ“Š å¯¼å‡ºCSVæ ¼å¼æ•°æ®
   * 
   * @param words - è¯é¢‘æ•°æ®æ•°ç»„
   * @returns Promise<void> - å¼‚æ­¥å¯¼å‡ºæ“ä½œ
   * 
   * ğŸ“‹ CSVæ ¼å¼:
   * - æ ‡å‡†é€—å·åˆ†éš”æ ¼å¼
   * - å®Œæ•´çš„å­—æ®µä¿¡æ¯
   * - æ­£ç¡®çš„ç¼–ç å¤„ç†
   * - Excelå…¼å®¹æ€§æ”¯æŒ
   */
  private async exportAsCSV(words: WordEntry[]): Promise<void> {
    const headers = ['Word', 'Frequency', 'Articles', 'Stemmed']
    const rows = words.map(word => [
      word.word,
      word.frequency.toString(),
      word.articles.join(';'),
      word.stemmed || ''
    ])

    const csvContent = [headers, ...rows]
      .map(row => row.map(cell => `"${cell}"`).join(','))
      .join('\n')

    this.downloadFile(csvContent, 'word-frequency-analysis.csv', 'text/csv')
  }

  /**
   * ğŸ“„ å¯¼å‡ºJSONæ ¼å¼æ•°æ®
   * 
   * @param words - è¯é¢‘æ•°æ®æ•°ç»„
   * @returns Promise<void> - å¼‚æ­¥å¯¼å‡ºæ“ä½œ
   * 
   * ğŸ“‹ JSONæ ¼å¼:
   * - ç»“æ„åŒ–æ•°æ®æ ¼å¼
   * - å®Œæ•´çš„å…ƒæ•°æ®
   * - æ—¶é—´æˆ³ä¿¡æ¯
   * - ç»Ÿè®¡æ‘˜è¦åŒ…å«
   */
  private async exportAsJSON(words: WordEntry[]): Promise<void> {
    const data = {
      exportedAt: new Date().toISOString(),
      statistics: this.getStatistics(words),
      words: words.slice(0, 100) // Limit for file size
    }

    const jsonContent = JSON.stringify(data, null, 2)
    this.downloadFile(jsonContent, 'word-frequency-analysis.json', 'application/json')
  }

  /**
   * ğŸ“‘ å¯¼å‡ºPDFæ ¼å¼æ•°æ®ï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰
   * 
   * @param words - è¯é¢‘æ•°æ®æ•°ç»„
   * @returns Promise<void> - å¼‚æ­¥å¯¼å‡ºæ“ä½œ
   * 
   * ğŸ“‹ PDFç‰¹ç‚¹:
   * - ä¸“ä¸šæ–‡æ¡£æ ¼å¼
   * - æ‰“å°å‹å¥½å¸ƒå±€
   * - å›¾è¡¨å¯è§†åŒ–
   * - å®Œæ•´çš„æŠ¥å‘Šç»“æ„
   * 
   * ğŸ›¡ï¸ å®ç°è¯´æ˜:
   * - æ¼”ç¤ºç¯å¢ƒæ¨¡æ‹Ÿå®ç°
   * - ç”Ÿäº§ç¯å¢ƒéœ€è¦PDFåº“
   * - æ¨èä½¿ç”¨jsPDFæˆ–PDFKit
   * - å¤æ‚å¸ƒå±€éœ€è¦ä¸“ä¸šåº“
   */
  private async exportAsPDF(words: WordEntry[]): Promise<void> {
    // This is a mock implementation
    // In a real app, you'd use a PDF library like jsPDF or PDFKit
    console.log('PDF export not implemented in demo')
    alert('PDF export would be implemented with a PDF library like jsPDF')
  }

  /**
   * ğŸ’¾ è§¦å‘æ–‡ä»¶ä¸‹è½½
   * 
   * @param content - æ–‡ä»¶å†…å®¹
   * @param filename - æ–‡ä»¶åç§°
   * @param mimeType - MIMEç±»å‹
   * 
   * ğŸ“‹ ä¸‹è½½æœºåˆ¶:
   * - Blobå¯¹è±¡åˆ›å»º
   * - URLä¸´æ—¶ç”Ÿæˆ
   * - è‡ªåŠ¨ä¸‹è½½è§¦å‘
   * - èµ„æºæ¸…ç†å›æ”¶
   * 
   * ğŸ›¡ï¸ æµè§ˆå™¨å…¼å®¹:
   * - ç°ä»£æµè§ˆå™¨æ”¯æŒ
   * - å®‰å…¨ç­–ç•¥éµå¾ª
   * - ç”¨æˆ·æƒé™æ£€æŸ¥
   * - é”™è¯¯çŠ¶æ€å¤„ç†
   */
  private downloadFile(content: string, filename: string, mimeType: string): void {
    const blob = new Blob([content], { type: mimeType })
    const url = URL.createObjectURL(blob)
    
    const a = document.createElement('a')
    a.href = url
    a.download = filename
    a.click()
    
    URL.revokeObjectURL(url)
  }
}

/**
 * ğŸ­ è¯é¢‘åˆ†ææœåŠ¡å•ä¾‹å®ä¾‹
 * 
 * ğŸ“‹ å•ä¾‹ä¼˜åŠ¿:
 * - å…¨å±€ä¸€è‡´æ€§ä¿è¯
 * - ç¼“å­˜æ•°æ®å…±äº«
 * - èµ„æºä½¿ç”¨ä¼˜åŒ–
 * - é…ç½®ç»Ÿä¸€ç®¡ç†
 */
export const wordFrequencyService = new WordFrequencyService()
export default wordFrequencyService